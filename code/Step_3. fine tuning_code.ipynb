{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/ssarkar445/huggingface-tinyllama-finetune-peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig,AutoPeftModelForCausalLM,PeftModel\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,TrainingArguments,pipeline\n",
    "from trl import SFTTrainer\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myconfig():\n",
    "    data_path = r\"/home/work/lib_data/hjy/finetuning_data_v4.pkl\"\n",
    "    access_token = \"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\"\n",
    "    model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    final_model = \"KIML/llama3_8B_hjy_v2\"\n",
    "    cache_dir = r\"/home/work/lib_data/Llama3\"\n",
    "    save_path = r\"/home/work/lib_data/hjy/finetuned_model\"\n",
    "    result_path_not_late = r\"/home/work/lib_data/hjy/results_not_late.pkl\"\n",
    "    result_path_late = r\"/home/work/lib_data/hjy/results_late.pkl\"\n",
    "    \n",
    "    result_path_not_late_2 = r\"/home/work/lib_data/hjy/results_not_late_2.pkl\"\n",
    "    result_path_late_2 = r\"/home/work/lib_data/hjy/results_late_2.pkl\"\n",
    "    seed = 117\n",
    "mcfg = myconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "finetuning_data_v3 = pd.read_pickle(mcfg.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistance for financial and commercial analysis<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "when the customer's conditions are as follows:\n",
      "Amount of given credit in NT dollars: 100000\n",
      "Gender: 2\n",
      "Education: 1\n",
      "Marital status: 1\n",
      "Age: 30\n",
      "Repayment status in June, 2005: 0\n",
      "Repayment status in May, 2005: 0\n",
      "Repayment status in April, 2005: 0\n",
      "Amount of bill statement in June, 2005: 0\n",
      "Amount of bill statement in May, 2005: 0\n",
      "Amount of bill statement in April, 2005: 2994\n",
      "Amount of previous payment in June, 2005: 0\n",
      "Amount of previous payment in May, 2005: 2995\n",
      "Amount of previous payment in April, 2005: 0\n",
      "Will the customer be late on October 2005 payment?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "Yes, this customer will default on his/her credit card payment. <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(finetuning_data_v3[\"prompt\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, eval_df = train_test_split(\n",
    "    finetuning_data_v3,\n",
    "    test_size=0.2,\n",
    "    random_state=mcfg.seed,\n",
    "    stratify=finetuning_data_v3['Default Payment Next Month']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_df[\"prompt\"]))\n",
    "eval_dataset = Dataset.from_pandas(pd.DataFrame(eval_df[\"prompt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(mcfg.access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_and_model(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True, cache_dir=mcfg.cache_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config, device_map=\"auto\", cache_dir=mcfg.cache_dir)\n",
    "    model.config.use_cache=False\n",
    "    model.config.pretraining_tp=1\n",
    "    return model,tokenizer\n",
    "\n",
    "model, tokenizer = get_tokenizer_and_model(mcfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,lora_alpha=16,lora_dropout=0.05,bias=\"none\",task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=mcfg.save_path,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=50,\n",
    "    # max_steps=200,\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=False,\n",
    "        max_seq_length=1024\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(mcfg.model,torch_dtype=torch.float16, load_in_8bit=False,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             cache_dir=mcfg.cache_dir)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "                                       '/home/work/lib_data/hjy/finetuned_model/checkpoint-275',\n",
    "                                       from_transformers=True,\n",
    "                                       device_map={\"\":0}\n",
    "                                       )\n",
    "\n",
    "model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(mcfg.final_model,token=mcfg.access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(mcfg.model)\n",
    "tokenizer.push_to_hub(mcfg.final_model,token=mcfg.access_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
